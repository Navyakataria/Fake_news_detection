# -*- coding: utf-8 -*-
"""Fake News Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fq9HWH_3s6GMHbXCFZ_gBw65FxBJmuuK

About the Dataset:

1. id: unique id for a news article
2. title: the title of a news article
3. author: author of the news article
4. text: the text of the article; could be incomplete
5. label: a label that marks whether the news article is real or fake:
           1: Fake news
           0: real News

This is a binary classification project so we will use a logistic regression model.

Importing the Dependencies
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np #numpy arrays
import pandas as pd #dataframes
import matplotlib.pyplot as plt #data visulatization
import re #regular expression (useful for searching text in document)
from nltk.corpus import stopwords #don't add much value to our data - stopwords
from nltk.stem.porter import PorterStemmer #to perform stemming
#Stemming takes a word and removes the prefix and suffix of that word, giving only the root
from sklearn.feature_extraction.text import TfidfVectorizer #converting text into
#feature vectors
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

import nltk
nltk.download('stopwords')

# printing the stopwords in English
print(stopwords.words('english'))

"""**Data Pre-processing**"""

# loading the dataset to a pandas DataFrame
news_dataset = pd.read_csv('/content/drive/MyDrive/DRDO project work/FakeNews Detection/train.csv')

news_dataset.shape

# print the first 5 rows of the dataframe
news_dataset.head()

# counting the number of missing values in the dataset
news_dataset.isnull().sum()

# replacing the null values with empty string
news_dataset = news_dataset.fillna('')

news_dataset.isnull().sum()

# merging the author name and news title
news_dataset['content'] = news_dataset['author']+' '+news_dataset['title']

print(news_dataset['content'])

# separating the data & label
X = news_dataset.drop(columns='label', axis=1)
Y = news_dataset['label']

print(X)
print(Y)

"""**Stemming:**

Stemming is the process of reducing a word to its Root word

example:
actor, actress, acting --> act
"""

port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content) #removes everything which is not in a-z or A-Z (like ,) - data cleaning
    stemmed_content = stemmed_content.lower() #converting all alphabets to lower case letters
    stemmed_content = stemmed_content.split() #converting to list
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    #performing stemming on words which are not stopwords
    stemmed_content = ' '.join(stemmed_content) #joining all the words by spaces in between them (list to string)
    return stemmed_content

news_dataset['content'] = news_dataset['content'].apply(stemming)

print(news_dataset['content'])

#separating the data and label
X = news_dataset['content'].values
Y = news_dataset['label'].values
#we are not using the text column of the initial dataset

print(X)

print(Y)

Y.shape

# Computer cannot understand text so that is why we are converting.
# Converting the textual data to numerical data
vectorizer = TfidfVectorizer() #counts the number of times a particular word is repeating
#in a document, it assigns a particular numerical value to that word. - term frequency
#idf - inverse document frequency, sometimes a word which is repeated several times
#does not have meaning in it. Suppose we are analyzing the reviews of a movie avengers
#so every review will contain the word avengers, but it is insignificant for our analyzing.
#So idf reduces this words importance value.
vectorizer.fit(X)
X = vectorizer.transform(X)

print(X)

"""Splitting the dataset to training & test data"""

#test data should be 20%
#Startify ensures that there is equal proportion of seperation between real and fake news
#as was in the original dataset
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)

"""**Logistic Regression**

Training the Model: Logistic Regression
"""

model_lr = LogisticRegression()

model_lr.fit(X_train, Y_train)

"""Accuracy Score"""

# accuracy score on the training data
X_train_prediction = model_lr.predict(X_train)
training_data_accuracy_lr = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy_lr)

# accuracy score on the test data
X_test_prediction = model_lr.predict(X_test)
test_data_accuracy_lr = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ', test_data_accuracy_lr)

"""Confusion Matrix"""

# calculate the confusion matrix
conf_mat = confusion_matrix(Y_test, X_test_prediction)

# print the confusion matrix
print("Confusion Matrix:")
print(conf_mat)

"""Classification Report"""

# calculate the classification report
report = classification_report(Y_test, X_test_prediction)
print("Classification Report:")
print(report)

"""**Naive Bayes**"""

model_nb = MultinomialNB()

model_nb.fit(X_train, Y_train)

"""Accuracy Score"""

# accuracy score on the training data
X_train_prediction = model_nb.predict(X_train)
training_data_accuracy_nb = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy_nb)

# accuracy score on the test data
X_test_prediction = model_nb.predict(X_test)
test_data_accuracy_nb = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ', test_data_accuracy_nb)

"""Confusion Matrix"""

# calculate the confusion matrix
conf_mat = confusion_matrix(Y_test, X_test_prediction)

# print the confusion matrix
print("Confusion Matrix:")
print(conf_mat)

"""Classification Report"""

# calculate the classification report
report = classification_report(Y_test, X_test_prediction)
print("Classification Report:")
print(report)

"""**Decision Tree**"""

model_dt = DecisionTreeClassifier()

model_dt.fit(X_train, Y_train)

"""Accuracy Score"""

# accuracy score on the training data
X_train_prediction = model_dt.predict(X_train)
training_data_accuracy_dt = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy_dt)

# accuracy score on the test data
X_test_prediction = model_dt.predict(X_test)
test_data_accuracy_dt = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ', test_data_accuracy_dt)

"""Confusion Matrix"""

# calculate the confusion matrix
conf_mat = confusion_matrix(Y_test, X_test_prediction)

# print the confusion matrix
print("Confusion Matrix:")
print(conf_mat)

"""Classification Report"""

# calculate the classification report
report = classification_report(Y_test, X_test_prediction)
print("Classification Report:")
print(report)

"""**Conclusion**"""

# create a list of model names and their corresponding accuracy scores
models = ['Logistic Regression', 'Naive Bayes', 'Decision Trees']
accuracy_scores = [test_data_accuracy_lr, test_data_accuracy_nb, test_data_accuracy_dt]

# create a bar graph
plt.figure(figsize=(10, 6))  # increase figure size for better readability
plt.bar(models, accuracy_scores, color=['blue', 'green', 'red'])  # use different colors for each bar
plt.xlabel('Model')
plt.ylabel('Accuracy Score')
plt.title('Comparison of Model Accuracy Scores')
plt.ylim(0.9, 1)  # set y-axis limit to 0.9-1 for better visualization
plt.xticks(rotation=45)  # rotate x-axis labels for better readability

# add a horizontal line to indicate the highest accuracy score
max_accuracy = max(accuracy_scores)
plt.axhline(y=max_accuracy, color='black', linestyle='--', label='Highest Accuracy')

# add a legend
plt.legend()

# show the plot
plt.tight_layout()
plt.show()

#This bag graph shows us that the maximum accuracy is provided by Decision Trees Model.

"""Making a Predictive System"""

X_new = X_test[3]

prediction = model_dt.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Real')
else:
  print('The news is Fake')

print(Y_test[3])

